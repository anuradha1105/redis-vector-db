# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

!pip install --quiet redis openai python-dotenv

import redis

r = redis.Redis(
    host="redis-19387.c232.us-east-1-2.ec2.redns.redis-cloud.com",
    port=19387,
    password="******************************",
    ssl=False,             # ðŸ‘ˆ turn OFF TLS
    decode_responses=False
)

print("Ping Redis:", r.ping())

!pip install -U "redis>=5.0.0" redisvl

from redis.commands.search.field import TextField, VectorField
from redis.commands.search.index_definition import IndexDefinition, IndexType

import redis

r = redis.Redis(
    host="redis-19387.c232.us-east-1-2.ec2.redns.redis-cloud.com",
    port=19387,
    password="***********************",
    ssl=False,
    decode_responses=False
)

print("Ping Redis:", r.ping())

from redis.commands.search.field import TextField, VectorField
from redis.commands.search.index_definition import IndexDefinition, IndexType

INDEX_NAME = "idx:docs"
DOC_PREFIX = "doc:"
VECTOR_DIM = 1536
DISTANCE_METRIC = "COSINE"

# drop existing index if any
try:
    r.ft(INDEX_NAME).dropindex(delete_documents=False)
except Exception:
    pass

schema = (
    TextField("title"),
    TextField("content"),
    VectorField(
        "embedding",
        "HNSW",
        {
            "TYPE": "FLOAT32",
            "DIM": VECTOR_DIM,
            "DISTANCE_METRIC": DISTANCE_METRIC,
        },
    ),
)

r.ft(INDEX_NAME).create_index(
    schema,
    definition=IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.HASH),
)

print("âœ… Redis vector index created")

def get_embedding(text: str):
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=text
    )
    return resp.data[0].embedding

def float32_to_bytes(vec):
    return np.array(vec, dtype=np.float32).tobytes()

docs = [
    {
        "id": "1",
        "title": "Redis vector search",
        "content": "Redis Stack supports vector similarity search using RediSearch.",
    },
    {
        "id": "2",
        "title": "OpenAI embeddings",
        "content": "OpenAI embeddings convert text into numerical vectors for semantic search.",
    },
    {
        "id": "3",
        "title": "Caching embeddings in Redis",
        "content": "Embeddings can be cached in Redis to avoid recomputing them and speed up queries.",
    },
]

for d in docs:
    emb = get_embedding(d["content"])
    key = f"{DOC_PREFIX}{d['id']}"
    r.hset(
        key,
        mapping={
            "title": d["title"],
            "content": d["content"],
            "embedding": float32_to_bytes(emb),
        },
    )

print("âœ… Inserted", len(docs), "docs")

!pip install -U "redis>=5.0.0" python-dotenv openai

import redis

r = redis.Redis(
    host="redis-19387.c232.us-east-1-2.ec2.redns.redis-cloud.com",
    port=19387,
    password="*************************",
    ssl=False,
    decode_responses=False,
)

print("Ping Redis:", r.ping())

import os
from openai import OpenAI

# put your real key here
os.environ["OPENAI_API_KEY"] = "*************************"

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

EMBEDDING_MODEL = "text-embedding-3-small"  # 1536-dim

from redis.commands.search.field import TextField, VectorField
from redis.commands.search.index_definition import IndexDefinition, IndexType

INDEX_NAME = "idx:docs"
DOC_PREFIX = "doc:"
VECTOR_DIM = 1536
DISTANCE_METRIC = "COSINE"


try:
    r.ft(INDEX_NAME).dropindex(delete_documents=False)
except Exception:
    pass

schema = (
    TextField("title"),
    TextField("content"),
    VectorField(
        "embedding",
        "HNSW",
        {
            "TYPE": "FLOAT32",
            "DIM": VECTOR_DIM,
            "DISTANCE_METRIC": DISTANCE_METRIC,
        },
    ),
)

r.ft(INDEX_NAME).create_index(
    schema,
    definition=IndexDefinition(
        prefix=[DOC_PREFIX],
        index_type=IndexType.HASH,
    ),
)

print("âœ… Redis vector index created")

import numpy as np

def get_embedding(text: str):
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=text
    )
    return resp.data[0].embedding

def float32_to_bytes(vec):
    return np.array(vec, dtype=np.float32).tobytes()

docs = [
    {
        "id": "1",
        "title": "Redis vector search",
        "content": "Redis Stack supports vector similarity search using RediSearch.",
    },
    {
        "id": "2",
        "title": "OpenAI embeddings",
        "content": "OpenAI embeddings convert text into numerical vectors for semantic search.",
    },
    {
        "id": "3",
        "title": "Caching embeddings in Redis",
        "content": "Embeddings can be cached in Redis so we do not recompute them for every query.",
    },
]

for d in docs:
    emb = get_embedding(d["content"])
    key = f"{DOC_PREFIX}{d['id']}"
    r.hset(
        key,
        mapping={
            "title": d["title"],
            "content": d["content"],
            "embedding": float32_to_bytes(emb),
        },
    )

print("âœ… Inserted", len(docs), "docs into Redis")

!pip install -q sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('all-MiniLM-L6-v2')

def get_embedding(text: str):
    emb = model.encode(text)
    return emb.tolist()

sample = "Redis Stack supports vector similarity search."
vec = get_embedding(sample)
print("Embedding length:", len(vec))
print("First 5 numbers:", vec[:5])

from redis.commands.search.field import TextField, VectorField
from redis.commands.search.index_definition import IndexDefinition, IndexType

INDEX_NAME = "idx:docs"
DOC_PREFIX = "doc:"
VECTOR_DIM = 384            # new dimension
DISTANCE_METRIC = "COSINE"

try:
    r.ft(INDEX_NAME).dropindex(delete_documents=False)
except Exception:
    pass

schema = (
    TextField("title"),
    TextField("content"),
    VectorField(
        "embedding",
        "HNSW",
        {"TYPE": "FLOAT32", "DIM": VECTOR_DIM, "DISTANCE_METRIC": DISTANCE_METRIC},
    ),
)

r.ft(INDEX_NAME).create_index(
    schema,
    definition=IndexDefinition(prefix=[DOC_PREFIX], index_type=IndexType.HASH),
)

print("âœ… Redis vector index created (384-dim model)")

docs = [
    {"id": "1", "title": "Redis vector search", "content": "Redis Stack supports vector similarity search using RediSearch."},
    {"id": "2", "title": "Sentence Transformers", "content": "Sentence Transformers generate text embeddings locally without API keys."},
    {"id": "3", "title": "Caching embeddings", "content": "Embeddings can be cached in Redis to speed up semantic search queries."},
]

for d in docs:
    emb = get_embedding(d["content"])
    key = f"{DOC_PREFIX}{d['id']}"
    r.hset(
        key,
        mapping={
            "title": d["title"],
            "content": d["content"],
            "embedding": float32_to_bytes(emb),
        },
    )

print("âœ… Inserted", len(docs), "docs into Redis")

from redis.commands.search.query import Query

def query_redis(query_text: str, k: int = 3):
    q_emb = get_embedding(query_text)
    base_query = f"*=>[KNN {k} @embedding $vec_param AS score]"
    q = (
        Query(base_query)
        .sort_by("score")
        .return_fields("title", "content", "score")
        .paging(0, k)
        .dialect(2)
    )
    params = {"vec_param": float32_to_bytes(q_emb)}
    res = r.ft(INDEX_NAME).search(q, query_params=params)
    return res

# Test
results = query_redis("how can I use redis for semantic search?")
for i, doc in enumerate(results.docs, 1):
    print(i, doc.title, "-", doc.score)

"""I built a Redis Vector Database to store and search embeddings for semantic queries. Initially, I used OpenAIâ€™s text-embedding-3-small model to generate the vectors, but ran into a RateLimitError since my OpenAI account had no credits left.

To get around that, I switched to a free alternative â€” the all-MiniLM-L6-v2 model from Hugging Face. It runs locally in Google Colab and doesnâ€™t require an API key, which made things a lot easier.

I updated the Redis index to support 384 dimensions, and all the documents were successfully stored. When I tested it with semantic queries, Redis returned accurate and relevant matches.

Overall, the project successfully created a working vector database with cached embeddings in Redis, making it possible to handle user queries efficiently.
"""

import json
with open("redis_vector_db.ipynb") as f:
    json.load(f)
print("âœ… Notebook is valid JSON")